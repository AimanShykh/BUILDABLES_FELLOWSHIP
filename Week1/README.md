# Week 1 ‚Äì Introduction to LLMs, Transformers & Tokenization

This week was focused on understanding the **foundations of Large Language Models (LLMs)**, the **Transformer architecture**, and the concept of **tokenization**. To apply these concepts, I built a **Text Summarization Web App with Tokenization Analysis**.

---

## üìñ Topics Covered

* **Large Language Models (LLMs)**

  * What LLMs are and how they process language
  * Their applications in tasks like summarization, translation, and chatbots

* **Transformer Architecture**

  * Self-attention mechanism
  * Encoder-Decoder structure
  * Importance in modern NLP models like GPT and BERT

* **Tokenization**

  * Converting text into tokens for model input
  * Word-level vs subword vs byte-pair encoding (BPE)
  * Impact of tokenization on model performance

---

## üõ†Ô∏è Project ‚Äì Text Summarization Web App with Tokenization Analysis

For the hands-on project, I developed a **web-based text summarization tool** along with **tokenization insights**.
The app allows users to input text, view the generated summary, and analyze how tokenization affects processing.

**Features:**

* TBD

**Tech Stack:**

* Python
* TBD

